<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ASRの言語モデルの統合方法 | Hattomo</title><meta name=keywords content><meta name=description content="音声認識(Audio Speech Recognition(ASR))の言語モデル(Language model(LM))の統合方法"><meta name=author content><link rel=canonical href=https://hattomo.github.io/posts/main/22/q4/1102-asr-lm/jp/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.2f95fbf78361c5758c2d203571e7cef9ffee638686d9c26960847370689ff217.css integrity="sha256-L5X794NhxXWMLSA1cefO+f/uY4aG2cJpYIRzcGif8hc=" rel="preload stylesheet" as=style><link rel=icon href=https://hattomo.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://hattomo.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://hattomo.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://hattomo.github.io/images/favicon/apple-icon-180x180.png><link rel=mask-icon href=https://hattomo.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><link rel=stylesheet href=https://hattomo.github.io/css/syntax.css><link rel=stylesheet href=https://hattomo.github.io/css/blogcard.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-352SDD97BP"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-352SDD97BP",{anonymize_ip:!1})}</script><meta property="og:title" content="ASRの言語モデルの統合方法"><meta property="og:description" content="音声認識(Audio Speech Recognition(ASR))の言語モデル(Language model(LM))の統合方法"><meta property="og:type" content="article"><meta property="og:url" content="https://hattomo.github.io/posts/main/22/q4/1102-asr-lm/jp/"><meta property="og:image" content="https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/componentfusion.webp"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-07T13:08:36+09:00"><meta property="article:modified_time" content="2022-11-13T14:26:01+09:00"><meta property="og:site_name" content="Hattomo"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/componentfusion.webp"><meta name=twitter:title content="ASRの言語モデルの統合方法"><meta name=twitter:description content="音声認識(Audio Speech Recognition(ASR))の言語モデル(Language model(LM))の統合方法"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://hattomo.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Main","item":"https://hattomo.github.io/posts/main/"},{"@type":"ListItem","position":4,"name":"ASRの言語モデルの統合方法","item":"https://hattomo.github.io/posts/main/22/q4/1102-asr-lm/jp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ASRの言語モデルの統合方法","name":"ASRの言語モデルの統合方法","description":"音声認識(Audio Speech Recognition(ASR))の言語モデル(Language model(LM))の統合方法","keywords":[""],"articleBody":"はじめに 音声認識技術は、スマートフォンの音声アシスタントや翻訳アプリなどを通して広く利用されている。近年の音声認識では深層学習を用いたConformerやContextNetなどのモデルが精度を大きく改善している。しかし、音声認識の精度は音声認識を行うモデルに言語モデルを統合することで改善することができることが知られている。言語モデルとは、ある時刻\\(t\\)の情報をもとに時刻\\(t+1\\)の出力を予測するモデルである。今回は、音声認識モデルの言語モデルの統合方法について説明する。\n言語モデルの統合方法 音声認識において、言語モデルの統合方法はshallow fusion, deep fusion, cold fusion, component fusionの4つがある。それぞれの統合方法の特徴を以下に示す。COMPONENT FUSION: LEARNING REPLACEABLE LANGUAGE MODEL COMPONENT FOR END-TO-END SPEECH RECOGNITION SYSTEMに違いがわかりやすく掲載されていたので図はそこから引用する。\nまず、言語モデルがない音声認識モデルの図を示す。 ここに言語モデルを足していくことになる。\nShallow fusion shallow fusionは最も多くのモデルに利用されているメジャーな方法である。 モデルの図を示す。 音声認識モデル部分には手を加えることなく、言語モデルを統合している。それぞれの出力を行った後言語モデルからの出力を\\(\\beta\\)倍して足している。 式は以下のようになる。\n\\[y_{t} = arg max(\\log{(y_{t}^{LAS})} + \\beta log{(y_{t}^{LM})})\\]\nDeep fusion Deep fustionは、言語モデルを内部の特徴量の段階で統合したモデルである。\n\\[ \\begin{aligned} g_t \u0026= sigmoid(U_gs_t^{LM}+b) \\\\ \\hat{h}_{t}^{att} \u0026= [h_t^{att};g_ts_t^{LM}] \\\\ y_t \u0026= softmax(W_o^’\\hat{h}_t^{att}) \\end{aligned} \\]\nここで,\\([x;y]\\)は\\(x\\)と\\(y\\)をconcatしたものを表している。 \\(g_t\\)は\\(U_g\\)によって調整されるパラメータであり、言語モデルの出力\\(s_t^{LM}\\)の情報をそれぞれのパラメータについてどれだけ利用するのかを決めている。その後、ASRモデルからの出力とconcatして\\(y_t\\)を計算している。\nCold fusion \\[ \\begin{aligned} h_t^{LM} \u0026= DNN(l_t^{LM}) \\\\ g_t \u0026= sigmoid(U_g[h_t^{LM};h_t^{att}]+b) \\\\ \\hat{h}_t^{att} \u0026= [h_t^{att};g_th_t^{LM}] \\\\ y_t \u0026= softmax(W_o^’\\hat{h}_t^{att}) \\end{aligned} \\]\ncold fusion では、言語モデルを利用する際、言語モデルの特徴量からのみで\\(g_t\\)を計算していたが、deep fusionではASRモデルからの情報も利用して利用する言語モデルの特徴量を決定する。\ncomponent fusion cold fustionをベースに言語モデルを切り離した方法である。まず初めに、言語モデルをASRモデルのラベルによって学習する。これによって高速で学習し、学習データのドメインにも対応できる。また、言語モデル自体を取り替えることもできる。また、このモデルでは言語モデルの特徴量を早期に結合しているため、よりASRモデルの浅い段階から学習に影響を与えるよう改良されている。\nReference Towards better decoding and language model integration in sequence to sequence models Cold Fusion: Training Seq2Seq Models Together with Language Models On using monolingual corpora in neural machine translation COMPONENT FUSION: LEARNING REPLACEABLE LANGUAGE MODEL COMPONENT FOR END-TO-END SPEECH RECOGNITION SYSTEM ","wordCount":"1119","inLanguage":"en","image":"https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/componentfusion.webp","datePublished":"2022-11-07T13:08:36+09:00","dateModified":"2022-11-13T14:26:01+09:00","author":[{"@type":"Person","name":""}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://hattomo.github.io/posts/main/22/q4/1102-asr-lm/jp/"},"publisher":{"@type":"Organization","name":"Hattomo","logo":{"@type":"ImageObject","url":"https://hattomo.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://hattomo.github.io accesskey=h title="Hattomo (Alt + H)">Hattomo</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://hattomo.github.io/search/ title="Search 🔎 (Alt + /)" accesskey=/><span>Search 🔎</span></a></li><li><a href=https://hattomo.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://hattomo.github.io/general/about title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://hattomo.github.io>Home</a>&nbsp;»&nbsp;<a href=https://hattomo.github.io/posts/>Posts</a>&nbsp;»&nbsp;<a href=https://hattomo.github.io/posts/main/>Main</a></div><h1 class=post-title>ASRの言語モデルの統合方法</h1><div class=post-description>音声認識(Audio Speech Recognition(ASR))の言語モデル(Language model(LM))の統合方法</div><div class=post-meta><span title='2022-11-07 13:08:36 +0900 +0900'>November 7, 2022</span>&nbsp;·&nbsp;&#8635; November 13, 2022&nbsp;·&nbsp;3 min&nbsp;·&nbsp;1119 words</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e3%81%af%e3%81%98%e3%82%81%e3%81%ab aria-label=はじめに>はじめに</a></li><li><a href=#%e8%a8%80%e8%aa%9e%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e7%b5%b1%e5%90%88%e6%96%b9%e6%b3%95 aria-label=言語モデルの統合方法>言語モデルの統合方法</a><ul><li><a href=#shallow-fusion aria-label="Shallow fusion">Shallow fusion</a></li><li><a href=#deep-fusion aria-label="Deep fusion">Deep fusion</a></li><li><a href=#cold-fusion aria-label="Cold fusion">Cold fusion</a></li><li><a href=#component-fusion aria-label="component fusion">component fusion</a></li></ul></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><h2 id=はじめに>はじめに<a hidden class=anchor aria-hidden=true href=#はじめに>#</a></h2><p>音声認識技術は、スマートフォンの音声アシスタントや翻訳アプリなどを通して広く利用されている。近年の音声認識では深層学習を用いたConformerやContextNetなどのモデルが精度を大きく改善している。しかし、音声認識の精度は音声認識を行うモデルに言語モデルを統合することで改善することができることが知られている。言語モデルとは、ある時刻\(t\)の情報をもとに時刻\(t+1\)の出力を予測するモデルである。今回は、音声認識モデルの言語モデルの統合方法について説明する。</p><h2 id=言語モデルの統合方法>言語モデルの統合方法<a hidden class=anchor aria-hidden=true href=#言語モデルの統合方法>#</a></h2><p>音声認識において、言語モデルの統合方法は<code>shallow fusion</code>, <code>deep fusion</code>, <code>cold fusion</code>, <code>component fusion</code>の4つがある。それぞれの統合方法の特徴を以下に示す。<a href=http://lxie.npu-aslp.org/papers/2019ICASSP-ChanghaoShan-LM.pdf>COMPONENT FUSION: LEARNING REPLACEABLE LANGUAGE MODEL COMPONENT FOR END-TO-END SPEECH RECOGNITION SYSTEM</a>に違いがわかりやすく掲載されていたので図はそこから引用する。</p><p>まず、言語モデルがない音声認識モデルの図を示す。
<img loading=lazy src=https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/baseline.webp alt=baseline>
ここに言語モデルを足していくことになる。</p><h3 id=shallow-fusion>Shallow fusion<a hidden class=anchor aria-hidden=true href=#shallow-fusion>#</a></h3><p>shallow fusionは最も多くのモデルに利用されているメジャーな方法である。
モデルの図を示す。
<img loading=lazy src=https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/shallowfusion.webp alt=shallowfusion>
音声認識モデル部分には手を加えることなく、言語モデルを統合している。それぞれの出力を行った後言語モデルからの出力を\(\beta\)倍して足している。
式は以下のようになる。</p><p>\[y_{t} = arg max(\log{(y_{t}^{LAS})} + \beta log{(y_{t}^{LM})})\]</p><h3 id=deep-fusion>Deep fusion<a hidden class=anchor aria-hidden=true href=#deep-fusion>#</a></h3><p>Deep fustionは、言語モデルを内部の特徴量の段階で統合したモデルである。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/deepfusion.webp alt=deepfusion></p><p>\[
\begin{aligned}
g_t &= sigmoid(U_gs_t^{LM}+b) \\
\hat{h}_{t}^{att} &= [h_t^{att};g_ts_t^{LM}] \\
y_t &= softmax(W_o^&rsquo;\hat{h}_t^{att})
\end{aligned}
\]</p><p>ここで,\([x;y]\)は\(x\)と\(y\)をconcatしたものを表している。
\(g_t\)は\(U_g\)によって調整されるパラメータであり、言語モデルの出力\(s_t^{LM}\)の情報をそれぞれのパラメータについてどれだけ利用するのかを決めている。その後、ASRモデルからの出力とconcatして\(y_t\)を計算している。</p><h3 id=cold-fusion>Cold fusion<a hidden class=anchor aria-hidden=true href=#cold-fusion>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/coldfusion.webp alt=deepfusion></p><p>\[
\begin{aligned}
h_t^{LM} &= DNN(l_t^{LM}) \\
g_t &= sigmoid(U_g[h_t^{LM};h_t^{att}]+b) \\
\hat{h}_t^{att} &= [h_t^{att};g_th_t^{LM}] \\
y_t &= softmax(W_o^&rsquo;\hat{h}_t^{att})
\end{aligned}
\]</p><p>cold fusion では、言語モデルを利用する際、言語モデルの特徴量からのみで\(g_t\)を計算していたが、deep fusionではASRモデルからの情報も利用して利用する言語モデルの特徴量を決定する。</p><h3 id=component-fusion>component fusion<a hidden class=anchor aria-hidden=true href=#component-fusion>#</a></h3><p><img loading=lazy src=https://raw.githubusercontent.com/Hattomo/blog-image/main/posts/main/22/Q4/1102-asr-lm/componentfusion.webp alt=componentfusion></p><p>cold fustionをベースに言語モデルを切り離した方法である。まず初めに、言語モデルをASRモデルのラベルによって学習する。これによって高速で学習し、学習データのドメインにも対応できる。また、言語モデル自体を取り替えることもできる。また、このモデルでは言語モデルの特徴量を早期に結合しているため、よりASRモデルの浅い段階から学習に影響を与えるよう改良されている。</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://arxiv.org/pdf/1612.02695.pdf>Towards better decoding and language model integration in sequence to sequence models</a></li><li><a href=https://arxiv.org/pdf/1708.06426.pdf>Cold Fusion: Training Seq2Seq Models Together with Language Models</a></li><li><a href=https://arxiv.org/pdf/1503.03535.pdf>On using monolingual corpora in neural machine translation</a></li><li><a href=http://lxie.npu-aslp.org/papers/2019ICASSP-ChanghaoShan-LM.pdf>COMPONENT FUSION: LEARNING REPLACEABLE LANGUAGE MODEL COMPONENT FOR END-TO-END SPEECH RECOGNITION SYSTEM</a></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://hattomo.github.io/posts/main/22/q4/1111-rakuten/jp/><span class=title>« Next</span><br><span>Rakuten Moblieの2022Q3決算を見て黒字化を考える</span></a>
<a class=next href=https://hattomo.github.io/posts/main/22/q3/0715-hubert/jp/><span class=title>Prev »</span><br><span>HuBERT</span></a></nav></footer></article></main><footer class=footer><span><a href=general/about_site/>2023 Hattomo</a></span>
<span>&#183;</span>
<span><a href=https://hattomo.github.io/general/privacypolicy>Privacy Policy</a></span>
<span>&#183;</span>
<span><a href=https://hattomo.github.io/general/opensource_licenses.txt>OpenSourceLicenses</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>